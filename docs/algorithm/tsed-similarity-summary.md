# コード類似性評価論文の要約

https://arxiv.org/abs/2404.08817

## 論文タイトル

Revisiting Code Similarity Evaluation with Abstract Syntax Tree Edit Distance

## 概要

この論文は、コード類似性評価における抽象構文木（AST）編集距離の適用を探求した研究です。特に、TSED（Tree Similarity of Edit Distance）メトリクスを多様なプログラミング言語に拡張し、従来の評価手法との比較を行っています。

## 主要な貢献

1. **TSED（Tree Similarity of Edit Distance）メトリクスの拡張**

   - 元々 SQL 用だった TSED を 48 のプログラミング言語に適用可能に拡張
   - 新しいツールとして公開

2. **評価メトリクスの包括的比較**

   - TSED
   - GPT-4 類似性スコア
   - BLEU
   - Jaccard 類似度
   - 実行一致度

3. **主な発見**
   - TSED は従来の統計的メトリクス（BLEU 等）より実行一致度との相関が高い
   - GPT-4 の類似性スコアは効果的だが出力が不安定
   - AST パーサーの選択が TSED の性能に大きく影響

## 技術的アプローチ

### TSED の計算プロセス

1. **コード解析**: tree-sitter を使用してコードを AST に変換
2. **木編集距離計算**: APTED アルゴリズムを使用
3. **正規化**: 0-1 のスコアに正規化

### 計算式

```
Δ(G1, G2) = min_ops Σ w(op_i)
TSED = max{1 - δ/MaxNodes(G1, G2), 0}
```

## 実験結果

### 対象言語と性能

- Java、Python、JavaScript、TypeScript、Ruby、Kotlin で評価
- TSED と GPT 類似性は従来メトリクスより高い精度を示す

### 主要な数値結果（MBXP データセット）

| 言語       | TSED   | BLEU   | Jaccard | GPT-4  | 実行一致 |
| ---------- | ------ | ------ | ------- | ------ | -------- |
| Java       | 0.3746 | 0.2041 | 0.2733  | 0.8143 | 0.6550   |
| Python     | 0.1888 | 0.0843 | 0.2000  | 0.6751 | 0.6842   |
| JavaScript | 0.2037 | 0.0846 | 0.2037  | 0.6763 | 0.6811   |

## 制限事項

1. **GPT スコアの不安定性**

   - 同じ入力でも出力が変動
   - MSE: 約 0.05-0.06、MAE: 約 0.18-0.20

2. **TSED のパラメータ依存性**

   - 操作の重み（削除、挿入、リネーム）が結果に影響
   - 言語ごとに最適なパラメータが異なる可能性

3. **パーサー依存性**
   - tree-sitter と ANTLR で結果が大きく異なる
   - パーサーの品質が評価精度に直接影響

## 実用的な意義

1. **コード生成タスクの評価**

   - LLM が生成したコードの品質評価に有効
   - 実行結果だけでなく構造的な類似性も評価可能

2. **多言語対応**

   - 48 のプログラミング言語に対応
   - 言語固有の構造を考慮した評価が可能

3. **従来手法の改善**
   - BLEU や Jaccard 類似度より実行一致度との相関が高い
   - コードの構造的特徴をより正確に捉える

## まとめ

TSED は、コードの構造的類似性を評価する有効な手法として、従来の統計的手法を上回る性能を示しました。特に、実行結果との相関が高く、コード生成タスクの評価において実用的な指標となることが期待されます。ただし、GPT スコアの不安定性やパラメータ調整の必要性など、実用化に向けた課題も明らかになりました。
